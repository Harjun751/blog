<html>
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" />

        <title>üë®‚Äçüíª | Choosing An Ai Provider</title>
        <link rel="stylesheet" href="style.css" />
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Archivo&family=Chakra+Petch:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="/blog/assets/css/styles.css">
    </head>

    <body>
        <nav>
    <a href="/blog/"><header>HARVINDER ARJUN SINGH</header></a>
    <a class="link" href="/blog/about">about</a>
</nav>
        <article class="blog-post">
    <figure>
    <img class="hero" src="/blog/assets/blog-images/2026-01-15/idk.png" />
</figure>

<h1 id="choosing-an-ai-assistant-in-2026">Choosing an AI assistant in 2026</h1>
<p>Despite my best efforts to abstain from using AI chatbots and assistance, it seems like I can abstain no more. A course that I‚Äôm taking this semester requires <strong>heavy</strong> use of AI in an exploratory role, seeing what tools and workflows work best for software development and what don‚Äôt.</p>

<p>I‚Äôm not blind towards the industry I‚Äôm working towards - new hires are expected to be proficient in using AI tools before they‚Äôre even considered. You‚Äôre ‚Äúlosing out‚Äù if you‚Äôre NOT using assistants to help with your studying or work. Statements I‚Äôm sure you‚Äôve heard before. I‚Äôm not a luddite - I do believe that these tools <em>can</em> and even <em>will</em> seismically shift the way we go about work in a positive way. The reservations I have about AI is not the technology itself (Machine Learning is a wonderful field that has improved lives significantly - AI I believe is just another prong of that), but rather about the way these large corporations have gone about in attaining it.</p>

<p>Companies like OpenAI and Meta seem to frankly just not give a shit:</p>

<ul>
  <li>Stealing <a href="https://arstechnica.com/tech-policy/2025/12/openai-desperate-to-avoid-explaining-why-it-deleted-pirated-book-datasets/">books</a>, <a href="https://www.forbes.com/sites/bernardmarr/2023/08/08/is-generative-ai-stealing-from-artists/">art</a>, <a href="https://www.businessinsider.com/openai-chatgpt-generative-ai-stole-personal-data-lawsuit-children-medical-2023-6?op=1">information</a> to train models</li>
  <li>Recklessly building datacenters, <a href="https://www.forbes.com/sites/kensilverstein/2026/01/11/americas-ai-boom-is-running-into-an-unplanned-water-problem/">draining local potable water supplies</a></li>
  <li>Whatever the hell <a href="https://www.bloomberg.com/news/articles/2026-01-07/musk-s-grok-ai-generated-thousands-of-undressed-images-per-hour-on-x">grok is doing</a></li>
</ul>

<details>
<summary>An aside on water use</summary>
<p>
There's a lot of contention about AI water use. Why is it even an issue?

Well, many data centers use <a href="https://www.accountabilityconsole.com/newsletter/articles/ai-data-centers-and-potable-water/">potable water supplies</a> from municipal systems. These water bodies are limited in throughput. A spike in usage means that more water is evaporated (because many data centers use *evaporative cooling*) - this in turn results in more water being in the atmosphere rather than in water bodies. The water in the atmosphere is not always guaranteed to fall back to where it came from - it goes to oceans and other places. Build enough data centers and I think you get the picture.

It's not a scarcity issue - at this moment we have enough water. It's a resource allocation issue. This is an issue that can be fixed with proper planning. I believe that companies must be aware of their effect on the community and take that into account. That's why to me, it's ethically iffy to support companies that churn out data centers with reckless abandon.
</p>
</details>

<p>It seems that all of these companies providing LLMs are guilty as sin. It‚Äôs the modus operandi of Big Tech - iterate and develop at any cost, capture market share, and deal with the consequences afterward. And by dealing with consequences, I mean something like paying a <a href="https://www.reuters.com/technology/italy-fines-openai-15-million-euros-over-privacy-rules-breach-2024-12-20/">15 million Euro fine</a> as a company that recently did a <a href="https://fortune.com/2024/10/02/openai-officially-raises-6-6-billion-funding-deal-157-billion-valuation-sam-altman-thrive-capital/">6.6 billion US dollar funding round</a>. That‚Äôs about 0.26% of that by the way.</p>

<p>On top of that, I do have a personal vendetta; AI has made the internet a vastly less-useful resource. New webpages are inundated with AI slop and <a href="https://github.com/harry0703/MoneyPrinterTurbo">things like this</a> just make it worse. The internet seems rife with it, to the point where I‚Äôm questioning what happens when AI is trained on AI. A convergence of slop I believe.</p>

<h1 id="so-what-do-i-do">So what do I do?</h1>
<p>My main objective of this is to find a LLM provider that is <strong>ethical</strong>. That is,</p>
<ol>
  <li>Acknowledgement of environmental toll, and concrete steps taken to mitigate/offset it.</li>
  <li>Acknowledgement of rights holders (authors, artists) when it comes to training data.</li>
  <li>Safeguards against harmful material (radicalization, CSAM, etc). At the very least a model that won‚Äôt help you kill yourself. (optional)</li>
  <li>Privacy. Might be a stretch. (optional)</li>
</ol>

<p>Note that I‚Äôm not evaluating features or quality by any means.</p>

<h2 id="openai">OpenAI</h2>
<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/chatgpt.png" />
</figure>

<p><strong>Environment</strong> - Seem to actively downplay water usage and electricity. Sure, Sam Altman claims a query uses ‚Äú0.000085 gallons of water; roughly  one-fifteenth of a teaspoon‚Äù, but we all know that inference/running queries is <em>not</em> where most of the usage is. It‚Äôs in training. And there have been no acknowledgements/statements on that. On top of that, the statement does not address that some queries <a href="https://llmrefs.com/blog/chatgpt-system-prompt-leak">fire off additional queries</a>. We haven‚Äôt event talked about image generation yet. [FAIL]</p>

<p><strong>Training Material</strong> - Current lawsuits on <a href="https://news.bloomberglaw.com/ip-law/openai-risks-billions-as-court-weighs-privilege-in-copyright-row">copyright theft</a>. [FAIL]</p>

<p><strong>Safety</strong> - Several scandals about <a href="https://edition.cnn.com/2025/11/06/us/openai-chatgpt-suicide-lawsuit-invs-vis">ChatGPT encouraging suicide</a>. Remember when it got super sycophantic and ChatGPT-induced psychosis <a href="https://edition.cnn.com/2025/09/05/tech/ai-sparked-delusion-chatgpt">became a thing</a>? Yea that was fun. Either way, OpenAI does seem to be <a href="https://openai.com/safety/">making an effort</a> towards safety. Whether that will be adequate is for time to tell. [MEH]</p>

<h2 id="meta">Meta</h2>
<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/meta.png" />
</figure>

<p><strong>Environment</strong> - Meta releases a sustainability report every year. They at least seem to be <em>thinking</em> about it. Their (lofty) goal is to achieve <a href="https://sustainability.atmeta.com/">net zero emissions and become water postive by 2030</a>. But, they‚Äôre also planning on constructing <a href="https://ppc.land/meta-confronts-sustainability-tensions-amid-massive-ai-infrastructure-expansion/">data centers that consume electricity in the scale of gigawatts</a>. Who knows, maybe they‚Äôll do it. Either way, Meta‚Äôs data centers in the here and now still <a href="https://www.nytimes.com/2025/07/14/technology/meta-data-center-water.html">drain the taps for local residents</a> and <a href="https://www.cnbc.com/2025/11/26/ai-data-center-frenzy-is-pushing-up-your-electric-bill-heres-why.html">inflate energy bills</a>. As an aside, electricity bills increasing feels more like an ‚ÄúAmerica‚Äù problem than a ‚Äúdata center‚Äù problem. [MEH]</p>

<p><strong>Training Material</strong> - <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-staff-torrented-nearly-82tb-of-pirated-books-for-ai-training-court-records-reveal-copyright-violations">Here we go again</a> [FAIL]</p>

<p><strong>Safety</strong> - Meta has introduced <a href="https://about.fb.com/news/2025/10/teen-ai-safety-approach/">parental guards</a> in response to <a href="https://www.forbes.com/sites/douglaslaney/2025/08/17/alternate-approaches-to-ai-safeguards-meta-versus-anthropic/">leaked policies</a> that were shockingly permissive. Meta‚Äôs track record is not good, and I‚Äôm inclined to believe that this was done to keep the Chatbot as engaging as possible to maximize value. But that is my own bias - take it as you will after [FAIL]</p>

<h2 id="google">Google</h2>
<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/google.png" />
</figure>

<p><strong>Environment</strong> - Google seems to be generally transparent when it comes to the environmental toll of AI. This <a href="https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference">article</a> summarizes a <a href="https://arxiv.org/abs/2508.15734">techinal paper</a> that they‚Äôve released that discusses the footprint of AI. They discuss how their data center operations take into account of the local watershed, energy efficiency, and more. The article isn‚Äôt peer reviewed, and still this is only taking into account of inference. Additionally, their claims on water/energy consumption is only on the <em>median</em>. Color me skeptical. This is still leagues above Meta and especially OpenAI‚Äôs approach. I‚Äôll give it a meagre pass. [PASS]</p>

<p><strong>Training Material</strong> - I think you‚Äôd be hard-pressed to find a AI model/company that hasn‚Äôt <a href="https://www.cio.com/article/2069449/french-regulator-fines-google-271m-over-generative-ai-copyright-issue.html">faced</a> <a href="https://www.reuters.com/legal/litigation/google-sued-by-us-artists-over-ai-image-generator-2024-04-29/">lawsuits</a> regarding training materials. Couldn‚Äôt find anything about Google pirating their training material though, so I‚Äôll give them that. [MEH]</p>

<p><strong>Safety</strong> - Again, seems pretty <a href="https://deepmind.google/blog/advancing-geminis-security-safeguards/">transparent</a>. Their safeguards seem to be strict to a point where people complain online. Whether that‚Äôs good or bad is up to you and how you use it. [PASS]</p>

<h2 id="microsoft">Microsoft</h2>
<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/microsoft.png" />
</figure>

<p><strong>Environment</strong> - Microsoft currently seems to be at the forefront of tackling this data center issue - voluntarily taking on <a href="https://edition.cnn.com/2026/01/13/tech/microsoft-ai-data-centers-electricity-bills-plan">higher electricity bills</a> to prevent spikes in bills, furthermore pledging to cover costs of improving the existing grid. On top of that, a concrete plan to <a href="https://www.reuters.com/business/microsoft-launches-data-center-initiative-limit-power-costs-water-use-2026-01-13/">curb impact of data center water use</a> and efforts to decrease <a href="https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/">water usage to zero</a>. Microsoft has not released any data regarding the footprint of a prompt or training a model. Either way, they seem committed and transparent. [PASS]</p>

<p><strong>Training Material</strong> - More claims of <a href="https://www.reuters.com/sustainability/boards-policy-regulation/microsoft-sued-by-authors-over-use-books-ai-training-2025-06-25/">piracy</a>. They‚Äôve also announced <a href="https://blogs.microsoft.com/on-the-issues/2023/09/07/copilot-copyright-commitment-ai-legal-concerns/">‚ÄúCopilot Copyright Commitment‚Äù</a> which states that they will‚Äùassume responsibility for the potential legal risks involved‚Äù on using generated copilot output. This is insanely odd to me and almost akin to admitting that chatbot output is/potentially can be illegal on the grounds of infringement. But go off I guess. [FAIL]</p>

<p><strong>Safety</strong> - As per the norm there are <a href="https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy">safeguards and policies</a> which attempt to mitigate potential damage. Old case of it egging on self-harm, but can‚Äôt find much cases in 2025. They seem to be rather strict on mitigating these cases. [PASS]</p>

<h2 id="anthropic">Anthropic</h2>
<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/anthropic.png" />
</figure>

<p><strong>Environment</strong> - Anthropic‚Äôs concerns seem to be set on <a href="https://www.anthropic.com/news/build-ai-in-america">building out and expanding</a> data centers across the US. The statements and reports I can find online are mostly related to this. They seem to have a strong focus on America ‚Äúwinning‚Äù the AI race - and their vision to achieve that would involve relaxing zoning laws and building out more and more energy infrastructure. There is a distinct lack of talk about sustainability and environmental impact, especially as a company that claims to be a beacon of responsibility in AI. [BIG FAIL]</p>

<p><strong>Training Material</strong> - They got fined <a href="https://www.bbc.com/news/articles/c5y4jpg922qo">1.5 billion for piracy</a></p>

<p><strong>Safety</strong> - At the very least Anthropic seems to be the one championing AI safety - they have a <a href="https://www.anthropic.com/rsp-updates">responsible scaling policy</a> (nope it‚Äôs not about the environment) that details how they secure their bots. Their <a href="https://www.anthropic.com/news/core-views-on-ai-safety">core views on AI safety</a> read well and make sense. In the crop of AI companies, they do seem to be one of the more transparent ones in this sense. [PASS]</p>

<h2 id="deepseek">Deepseek</h2>

<figure>
    <img class="blog-img" src="/blog/assets/blog-images/2026-01-15/deepseek.png" />
</figure>

<p><strong>Environment</strong> - DeepSeek‚Äôs models seem to be more efficient during the training phase, but <a href="https://www.technologyreview.com/2025/01/31/1110776/deepseek-might-not-be-such-good-news-for-energy-after-all/">worse during inference</a>. They notably caused Nvidia shares to sink when they publicized the cost of training DeepSeek-V3, claiming that training the model required far less infrastructure than normal.They have not published any sustainability reports or have made statements about their data centers. I would err on the side of caution. [MEH?]</p>

<p><strong>Training Material</strong> - Deepseek says that data in the pre-training stage is <a href="https://archive.ph/TRiG3#selection-1137.172-1141.91">‚Äúmainly collected from publicly available online information and authorised third-party data‚Äù</a>. If this is true, it would be the only provider so far to actually get authorisation. However, I am unsure on the difference between the ‚Äúpre-training‚Äù stage and the ‚Äútraining‚Äù stage - the statement seems quite specifically worded. There really isn‚Äôt much information on this from Deepseek, so again, I‚Äôd err on the side of caution. [MEH?]</p>

<p><strong>Safety</strong> - It seems that Deepseek are lacking behind their contermporaries in this - their safety guardrails <a href="https://www.wired.com/story/deepseeks-ai-jailbreak-prompt-injection-attacks/">failed many tests</a> by researchers.</p>

<h1 id="conclusion">Conclusion</h1>
<p>Well if you‚Äôre using AI you just have to accept that these companies have profitted off pirating vast quanitites of material and/or using them without express permission. It‚Äôs actually quite hard to see AI without it - LLMs are almost rapacious in their hunger for training material, and feeding them the wealth of information that lives online and in our books seems like a natural step. But there has to have been a better way to achieve this than how these companies went about it.</p>

<p>Some companies are better for the environment than others, and surprisingly Microsoft pulled through on that one.</p>

<p>All of the companies have AI safeguards, with some of them being more on the forefront of research and safety features. But prompt injections and ‚Äújailbreak‚Äù prompts are still rife and are a problem that has not been solved yet. I‚Äôm not sure it ever will be solved without crippling usage. Again, you‚Äôll just have to accept that these tools are perhaps too powerful for our own good.</p>

<p>Privacy was not even talked about - data leakage has been reported in all major providers. Unless you‚Äôre using a enterprise-level subscription with separated information (microsoft offers this), it‚Äôs a risk that you will have to accept. Privacy-focused individuals should be using offline and self-hosted LLMs anyway.</p>

<p>Personally, I have a larger focus on mitigating the environmental toll that these companies can cause. Therefore I will be going with Microsoft‚Äôs Copilot.</p>

</article>

    </body>
</html>